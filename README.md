# ğŸƒ Blackjack Q-Learning Agent: Reinforcement Learning Mastery

<div align="center">
  
![image](https://github.com/user-attachments/assets/7d1987c1-08c3-4fff-9d30-31027b78ef69)
</div>

## ğŸŒŸ Projeto

Uma implementaÃ§Ã£o avanÃ§ada de aprendizado por reforÃ§o utilizando o algoritmo Q-Learning para resolver o ambiente de Blackjack do Gymnasium, demonstrando tÃ©cnicas sofisticadas de aprendizado de mÃ¡quina e estratÃ©gias de decisÃ£o adaptativas.

## ğŸ“Š VisÃ£o Geral TÃ©cnica

### Algoritmo
- **TÃ©cnica**: Q-Learning (Aprendizado por DiferenÃ§a Temporal)
- **Ambiente**: OpenAI Gymnasium Blackjack-v1
- **Objetivo**: Desenvolver uma estratÃ©gia Ã³tima para jogar Blackjack

### ğŸ§  CaracterÃ­sticas Principais
- ExploraÃ§Ã£o adaptativa via polÃ­tica Ã©psilon-gulosa
- AtualizaÃ§Ã£o dinÃ¢mica da tabela Q
- Decaimento exponencial da taxa de exploraÃ§Ã£o
- Tratamento robusto de estados e aÃ§Ãµes

## ğŸ”¬ Componentes Arquiteturais

### `BlackJackQLearningAgent`
Classe central que encapsula a lÃ³gica completa de aprendizado por reforÃ§o.

#### HiperparÃ¢metros
- `epsilon`: Taxa de exploraÃ§Ã£o inicial (0.2)
- `alpha`: Taxa de aprendizado (0.2)
- `gamma`: Fator de desconto (0.99)

#### MÃ©todos Chave
- `get_state_key()`: ConversÃ£o de estados
- `get_action()`: SeleÃ§Ã£o de aÃ§Ãµes
- `update()`: AtualizaÃ§Ã£o da polÃ­tica
- `train()`: Treinamento do agente

## ğŸ“ˆ MÃ©tricas e AnÃ¡lise

### AvaliaÃ§Ã£o de Desempenho
- Treinamento: 1,000,000 episÃ³dios
- MÃ©tricas calculadas:
  - Taxa de vitÃ³ria
  - Taxa de empate
  - Taxa de derrota

### VisualizaÃ§Ã£o
- GrÃ¡fico de mÃ©dia mÃ³vel de recompensas
- EvoluÃ§Ã£o do aprendizado ao longo dos episÃ³dios

## ğŸš€ DependÃªncias

### Bibliotecas Utilizadas
- `gymnasium`: Ambiente de aprendizado por reforÃ§o
- `numpy`: ComputaÃ§Ãµes numÃ©ricas
- `pandas`: ManipulaÃ§Ã£o de dados
- `matplotlib`: VisualizaÃ§Ã£o
- `tqdm`: Progresso de treinamento

## ğŸ’» InstalaÃ§Ã£o & ExecuÃ§Ã£o

### PrÃ©-requisitos
- Python 3.8+
- pip

### InstalaÃ§Ã£o
```bash
pip install gymnasium numpy pandas matplotlib tqdm
```

## ğŸ§® Detalhes MatemÃ¡ticos

### EquaÃ§Ã£o de AtualizaÃ§Ã£o Q-Learning
Q(s,a) â† Q(s,a) + Î± * [R + Î³ * max(Q(s')) - Q(s,a)]

Onde:
- Q(s,a): Valor da aÃ§Ã£o
- Î±: Taxa de aprendizado
- R: Recompensa
- Î³: Fator de desconto
- max(Q(s')): MÃ¡ximo valor Q para prÃ³ximo estado

## ğŸ” EstratÃ©gias Implementadas

### ExploraÃ§Ã£o vs ExplotaÃ§Ã£o
- PolÃ­tica Ã‰psilon-Gulosa
- Decaimento exponencial de Ã©psilon
- ExploraÃ§Ã£o inicial vs ExplotaÃ§Ã£o progressiva

### Tratamento de Estados
- Suporte a estados com Ã¡s utilizÃ¡vel
- Mapeamento dinÃ¢mico de estados para chaves

## ğŸ² DemonstraÃ§Ã£o

### Funcionalidades
- Treinamento do agente
- Plotagem de resultados
- AvaliaÃ§Ã£o de desempenho
- Modo de visualizaÃ§Ã£o de jogos

## ğŸ”¬ AnÃ¡lise de Complexidade

### EspaÃ§o
- O(n): Tabela Q cresce com nÃºmero de estados Ãºnicos
- Complexidade: Linear com estados explorados

### Tempo
- O(m * k): m = episÃ³dios, k = passos por episÃ³dio
- Treinamento: ConvergÃªncia em ~1,000,000 episÃ³dios

## ğŸ¦¾ PossÃ­veis ExtensÃµes
- Implementar Deep Q-Learning
- Adicionar funÃ§Ã£o de aproximaÃ§Ã£o de valor
- Experimentar outros algoritmos de RL
- Generalizar para outros jogos de casino

## ğŸ“ ContribuiÃ§Ãµes
Pull requests sÃ£o bem-vindos. Para mudanÃ§as importantes, abra um issue primeiro para discutir o que vocÃª gostaria de modificar.

## ğŸ“‹ LicenÃ§a
[MIT](https://choosealicense.com/licenses/mit/)

---
